{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install required packages with compatible versions\n",
        "!pip install \"langchain<0.1.0\" langchain-community langchain-google-genai chromadb sentence-transformers pypdf gradio"
      ],
      "metadata": {
        "id": "ya9KbeefYPjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import gradio as gr\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.prompts import PromptTemplate\n",
        "import os\n",
        "import tempfile\n"
      ],
      "metadata": {
        "id": "d9QRT9jLYT-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Global variables\n",
        "vectorstore = None\n",
        "qa_chain = None\n",
        "\n",
        "def process_pdf(pdf_file, api_key):\n",
        "    \"\"\"Process uploaded PDF and create vector store\"\"\"\n",
        "    global vectorstore, qa_chain\n",
        "\n",
        "    try:\n",
        "        # Validate inputs\n",
        "        if not api_key or len(api_key.strip()) == 0:\n",
        "            return \"âŒ Please enter a valid API key\"\n",
        "\n",
        "        if pdf_file is None:\n",
        "            return \"âŒ Please upload a PDF file\"\n",
        "\n",
        "        # Set API key\n",
        "        os.environ[\"GOOGLE_API_KEY\"] = api_key.strip()\n",
        "\n",
        "        # Handle file path (Gradio can pass either bytes or path)\n",
        "        if isinstance(pdf_file, bytes):\n",
        "            # Save uploaded file temporarily\n",
        "            with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:\n",
        "                tmp_file.write(pdf_file)\n",
        "                tmp_path = tmp_file.name\n",
        "        else:\n",
        "            # File is already a path\n",
        "            tmp_path = pdf_file\n",
        "\n",
        "        # Load PDF\n",
        "        loader = PyPDFLoader(tmp_path)\n",
        "        documents = loader.load()\n",
        "\n",
        "        if len(documents) == 0:\n",
        "            return \"âŒ No content found in PDF\"\n",
        "\n",
        "        # Split text into chunks\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=200\n",
        "        )\n",
        "        texts = text_splitter.split_documents(documents)\n",
        "\n",
        "        # Create embeddings\n",
        "        embeddings = HuggingFaceEmbeddings(\n",
        "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "        )\n",
        "\n",
        "        # Create vector store\n",
        "        vectorstore = Chroma.from_documents(\n",
        "            documents=texts,\n",
        "            embedding=embeddings\n",
        "        )\n",
        "\n",
        "        # Set up Gemini LLM - Fixed configuration\n",
        "        llm = ChatGoogleGenerativeAI(\n",
        "            model=\"gemini-1.5-flash\",\n",
        "            temperature=0.7,\n",
        "            google_api_key=api_key.strip(),\n",
        "            convert_system_message_to_human=True  # This is the key fix!\n",
        "        )\n",
        "\n",
        "        # Create RAG prompt template (compatible with RetrievalQA)\n",
        "        prompt_template = \"\"\"You are an assistant for question-answering tasks.\n",
        "                            Use the following pieces of retrieved context to answer the question.\n",
        "                            If you don't know the answer, say that you don't know.\n",
        "                            Keep the answer concise.\n",
        "\n",
        "                            Context:\n",
        "                            {context}\n",
        "\n",
        "                            Question: {question}\n",
        "\n",
        "                            Answer:\"\"\"\n",
        "\n",
        "        PROMPT = PromptTemplate(\n",
        "            template=prompt_template,\n",
        "            input_variables=[\"context\", \"question\"]\n",
        "        )\n",
        "\n",
        "        # Create QA chain with custom prompt\n",
        "        qa_chain = RetrievalQA.from_chain_type(\n",
        "            llm=llm,\n",
        "            chain_type=\"stuff\",\n",
        "            retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
        "            return_source_documents=True,\n",
        "            chain_type_kwargs={\"prompt\": PROMPT}\n",
        "        )\n",
        "\n",
        "        # Clean up temp file if we created one\n",
        "        if isinstance(pdf_file, bytes):\n",
        "            os.unlink(tmp_path)\n",
        "\n",
        "        return f\"âœ… PDF processed successfully! Loaded {len(documents)} pages with {len(texts)} chunks. You can now ask questions.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"âŒ Error processing PDF: {str(e)}\"\n",
        "\n",
        "def chat(message, history):\n",
        "    \"\"\"Handle chat messages\"\"\"\n",
        "    global qa_chain\n",
        "\n",
        "    if qa_chain is None:\n",
        "        return history + [[message, \"âš ï¸ Please upload and process a PDF first!\"]]\n",
        "\n",
        "    if not message or len(message.strip()) == 0:\n",
        "        return history\n",
        "\n",
        "    try:\n",
        "        result = qa_chain({\"query\": message})\n",
        "        return history + [[message, result[\"result\"]]]\n",
        "    except Exception as e:\n",
        "        return history + [[message, f\"âŒ Error: {str(e)}\"]]\n",
        "\n",
        "# Create Gradio interface\n",
        "with gr.Blocks(title=\"RAG PDF Chatbot\", theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# ðŸ“š RAG PDF Chatbot with Gemini\")\n",
        "    gr.Markdown(\"Upload a PDF and chat with it using Google's Gemini AI\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            api_key_input = gr.Textbox(\n",
        "                label=\"ðŸ”‘ Google API Key\",\n",
        "                type=\"password\",\n",
        "                placeholder=\"Enter your Gemini API key (starts with AIza...)\"\n",
        "            )\n",
        "            pdf_input = gr.File(\n",
        "                label=\"ðŸ“„ Upload PDF\",\n",
        "                file_types=[\".pdf\"]\n",
        "            )\n",
        "            upload_btn = gr.Button(\"ðŸš€ Process PDF\", variant=\"primary\")\n",
        "            status_output = gr.Textbox(label=\"ðŸ“Š Status\", interactive=False)\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            chatbot = gr.Chatbot(height=400, label=\"ðŸ’¬ Chat\")\n",
        "            msg = gr.Textbox(\n",
        "                label=\"Ask a question about your PDF\",\n",
        "                placeholder=\"Type your question here and press Enter...\"\n",
        "            )\n",
        "            clear = gr.Button(\"ðŸ—‘ï¸ Clear Chat\")\n",
        "\n",
        "    # Event handlers\n",
        "    upload_btn.click(\n",
        "        fn=process_pdf,\n",
        "        inputs=[pdf_input, api_key_input],\n",
        "        outputs=status_output\n",
        "    )\n",
        "\n",
        "    msg.submit(\n",
        "        fn=chat,\n",
        "        inputs=[msg, chatbot],\n",
        "        outputs=chatbot\n",
        "    ).then(\n",
        "        lambda: \"\",\n",
        "        outputs=msg\n",
        "    )\n",
        "\n",
        "    clear.click(lambda: [], None, chatbot, queue=False)\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "    ### ðŸ“– How to use:\n",
        "    1. Get a free API key from [Google AI Studio](https://aistudio.google.com/app/apikey)\n",
        "    2. Enter your API key above (keep it safe!)\n",
        "    3. Upload a PDF document\n",
        "    4. Click 'ðŸš€ Process PDF' and wait for confirmation\n",
        "    5. Start asking questions about your PDF!\n",
        "\n",
        "    ### âš¡ Features:\n",
        "    - RAG (Retrieval-Augmented Generation) for accurate answers\n",
        "    - Gemini 1.5 Flash for fast responses\n",
        "    - Free HuggingFace embeddings\n",
        "    - Processes PDFs of any size\n",
        "    - Using LangChain <0.1.0 for maximum compatibility\n",
        "    \"\"\")\n",
        "\n",
        "# Launch the app\n",
        "demo.launch(debug=True, share=True)"
      ],
      "metadata": {
        "id": "VW7oqzLTYXCa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}